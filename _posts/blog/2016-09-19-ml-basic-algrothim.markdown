---
layout: post
title:  "10个基本的机器学习算法!"
date:   2016-09-19 14:44:00 +0800
categories: blog
---

机器学习算法可以分为三个大类：**监督学习**、**无监督学习**、**强化学习**。其中:
>- **监督学习**对于有标签的特定数据集（训练集）是非常有效的，但是它需要对于其他的距离进行预测。
>- **无监督学习**对于在给定未标记的数据集（目标没有提前指定）上发现潜在关系是非常有用的。
>- **强化学习**介于这两者之间—它针对每次预测步骤（或行动）会有某种形式的反馈，但是没有明确的标记或者错误信息。本文主要介绍有关监督学习和无监督学习的10种算法。
  
---------
监督学习

1. **决策树（Decision Trees）：**

    决策树是一个决策支持工具，它使用树形图或决策模型以及序列可能性。包括各种偶然事件的后果、资源成本、功效。下图展示的是它的大概原理：
    ![decision-tree](/pics/ml-basic-algrothim/decision-tree.gif)
    
    从业务决策的角度来看，大部分情况下决策树是评估作出正确的决定的概率最不需要问是/否问题的办法。它能让你以一个结构化的和系统化的方式来处理这个问题，然后得出一个合乎逻辑的结论。

    **优点：**
    
    1. 能够处理不相关的特征；
    2. 在相对短的时间内能够对大型数据源做出可行且效果良好的分析；
    3. 计算简单，易于理解，可解释性强；
    4. 比较适合处理有缺失属性的样本。
    
    **缺点：**
    
    1. 忽略了数据之间的相关性；
    2. 容易发生过拟合（随机森林可以很大程度上减少过拟合）；
    3. 在决策树当中,对于各类别样本数量不一致的数据，信息增益的结果偏向于那些具有更多数值的特征。

2. **朴素贝叶斯分类(Naive Bayesian classification)：**

    朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。
    ![naive-bayesian-classification.jpg](/pics/ml-basic-algrothim/naive-bayesian-classification.jpg)
    
    它的现实使用例子有：
    >- 将一封电子邮件标记（或者不标记）为垃圾邮件
    >- 将一篇新的文章归类到科技、政治或者运动
    >- 检查一段文本表达的是积极情绪还是消极情绪
    >- 脸部识别软件
    
    朴素贝叶斯的思想十分简单，对于给出的待分类项，求出在此项出现的条件下各个类别出现的概率，以概率大小确定分类项属于哪个类别。
    
    **优点:**
    
    1. 朴素贝叶斯模型发源于古典数学理论，因此有着坚实的数学基础，以及稳定的分类效率；
    2. 算法较简单，常用于文本分类；
    3. 对小规模的数据表现很好，能够处理多分类任务，适合增量式训练。
    
    **缺点：**
    
    1. 需要计算先验概率；
    2. 对输入数据的表达形式很敏感；
    3. 分类决策存在错误率。
    
        ![bayesian-1.gif](/pics/ml-basic-algrothim/bayesian-1.gif)
    
3. **最小二乘法（Ordinary Least Squares Regression）：**

    如果你懂统计学的话，你可能以前听说过线性回归。最小二乘法是一种计算线性回归的方法。你可以把线性回归当做在一系列的点中画一条合适的直线的任务。有很多种方法可以实现这个，“最小二乘法”是这样做的 —你画一条线，然后为每个数据点测量点与线之间的垂直距离，并将这些全部相加，最终得到的拟合线将在这个相加的总距离上尽可能最小。
    ![ordinaryleast-suares-regression.jpg](/pics/ml-basic-algrothim/ordinaryleast-suares-regression.jpg)
    
    线性回归与逻辑回归不同，它是用于回归的，而不是用于分类。其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化。
    
    **优点：**
    >- 实现简单，计算简单；
    
    **缺点：**
    >- 不能拟合非线性数据。
    
4. **逻辑回归(Logistic Regression)：**

    逻辑回归是一种强大的统计方法，它能建模出一个二项结果与一个（或多个）解释变量。它通过估算使用逻辑运算的概率，测量分类依赖变量和一个（或多个）独立的变量之间的关系，这是累积的逻辑分布情况。
    ![logistic-regression.jpg](/pics/ml-basic-algrothim/logistic-regression.jpg)
    
    总的来说，逻辑回归可以用于以下几个真实应用场景：
    >- 信用评分
    >- 测量营销活动的成功率
    >- 预测某一产品的收入
    >- 特定某一天是否会发生地震
    
    **优点：**
    
    1. 实现简单，广泛地应用于工业问题上；
    2. 可以结合L2正则化解决多重共线性问题；
    3. 分类时计算量非常小，速度很快，存储资源低；
    
    **缺点：**
    
    1. 不能很好地处理大量多类特征或变量；
    2. 容易欠拟合，一般准确度较低；
    3. 对于非线性特征，需要进行转换；
    4. 当特征空间很大时，逻辑回归的性能不是很好；
    5. 只能处理两分类问题（在该基础上衍生出来的softmax可以用于多分类），且必须线性可分。
    
        ![logistic-1.jpg](/pics/ml-basic-algrothim/logistic-1.jpg)
    
5. **支持向量机（Support Vector Machine）：**

    SVM（Support Vector Machine）是二元分类算法。给定一组2种类型的N维的地方点，SVM（Support Vector Machine）产生一个（N - 1）维超平面到这些点分成2组。假设你有2种类型的点，且它们是线性可分的。 SVM（Support Vector Machine）将找到一条直线将这些点分成2种类型，并且这条直线会尽可能地远离所有的点。
    在规模方面，目前最大的使用支持向量机SVM（Support Vector Machine）（在适当修改的情况下）的问题是显示广告，人类剪接位点识别，基于图像的性别检测，大规模的图像分类等。
    ![SVM.jpg](/pics/ml-basic-algrothim/SVM.jpg)
    
6. **组合方法（Ensemble methods）：**

    组合方法是学习算法，它构建一系列分类，然后通过采取加权投票预测的方式来对新的数据点进行分类。原始的集成方法是贝叶斯平均法，但最近的算法包括对其纠错输出编码、套袋、加速等。
    那么组合方法如何运行的呢？为什么说它们比其他的模型要优秀？因为：
    >- 它们将偏差平均了：如果你将民主党派的民意调查和共和党的民意调查发在一起平均化，那么你将得到一个均衡的结果，且不偏向任何一方。
    >- 它们减少了差异：一堆模型的总结意见没有一个模型的单一意见那么嘈杂。在金融领域，这就是所谓的多元化 — 有许多股票组合比一个单独的股票的不确定性更少，这也为什么你的模型在数据多的情况下会更好的原因。
    >- 它们不太可能过度拟合：如果你有没有过度拟合的独立模型，你通过一个简单的方式（平均，加权平均，逻辑回归）对每个独立模型的预测进行结合，这样的话不太可能会出现过度拟合的情况。
    ![ensemble-methods.jpg](/pics/ml-basic-algrothim/ensemble-methods.jpg)
    
---------

7. **聚类算法（Clustering Algorithms）：**

    聚类是一种聚集对象的任务，例如：相比其他不同的组在同一组（集群）的对象彼此更为相似。
    每个聚类算法都是不同的，比如说有以下几种：
    >- 基于质心的算法
    >- 基于连接的算法
    >- 基于密度的算法
    >- 可能性
    >- 维度缩减
    >- 神经网络/深度学习
    ![clustering-algorithms.png](/pics/ml-basic-algrothim/clustering-algorithms.png)
    
    对于 **最近邻算法**
    
    **优点：**
    
    1. 对数据没有假设，准确度高；
    2. 可用于非线性分类；
    3. 训练时间复杂度为O(n)；
    4. 理论成熟，思想简单，既可以用来做分类也可以用来做回归。
    
    **缺点：**
    
    1. 计算量大；
    2. 需要大量的内存；
    3. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）。
    
8. **主成分分析（Principal Component Analysis，PCA）：**

    通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。
    一些主成分分析PCA程序的应用包括压缩、简化数据、可视化。注意，对于选择是否使用主成分分析领域知识是非常重要的。当数据是嘈杂的时候（所有的组件的主成分分析有相当高的方差），它是不适合的。
    ![PCA.jpg](/pics/ml-basic-algrothim/PCA.jpg)
    
9. **奇异值分解（Singular Value Decomposition）：**

    在线性代数中，SVD是一个非常复杂矩阵的因数分解。对于一个给定的m×n矩阵M，存在一个分解，M = UΣV，其中u和v是单一矩阵，Σ是对角矩阵。
    主成分分析PCA其是奇异值分解SVD的简单应用。在计算机视觉领域，第一人脸识别算法，运用主成分分析PCA其是奇异值分解SVD来代表面孔作为一个线性组合的“特征脸”，并对其做降维，然后通过简单的方法匹配合适的身份；虽然现代方法更复杂，但是许多人仍然依靠类似的技术。
    ![SVD.jpg](/pics/ml-basic-algrothim/SVD.jpg)
    
10. **独立成分分析（Independent Component Analysis）：**

    独立成分分析（Independent Component Analysis，ICA）是一种揭示构筑随机变量、技术测量、信号等隐藏因素的统计技术。ICA定义了所观察到的多变量数据生成模型，这通常是给定为一个大型数据库的样本。在该模型中，数据变量被假定为一些未知潜变量的线性混合，同时混合系统也仍然未知。潜变量被假定是非高斯和相互独立的，它们被称为所观察到的数据的独立分量。
    ![independent-component-analysis.jpg](/pics/ml-basic-algrothim/independent-component-analysis.jpg)
    
    ICA与PCA是相关的，但它更强大。在这些经典的方法完全失败的时候，ICA能够找到源头的潜在因素。它的应用包括数字图像、文档数据库、经济指标和心理测试。